{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 전기톱원숭이 팀\n",
    "\n",
    "# Private[0.76034] : ESM embedding(3 sequences) + Tabnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import random\n",
    "import pickle\n",
    "from itertools import product\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import esm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MaxAbsScaler, Normalizer, RobustScaler, StandardScaler,MinMaxScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "from pytorch_tabnet.metrics import Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Configuration and Hyperparameters\n",
    "- Embedding sequence modeling의 정확한 재현을 위해서는 Multi-GPU를 사용하고, Batch_size를 동일하게 적용해야함\n",
    "- 실제로 운용 시 batch_size를 크게 늘릴 수 있음\n",
    "- **Don't touch bathc size**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set visible device\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"0, 1, 2, 3\"\n",
    "\n",
    "\n",
    "CONFIG = {\n",
    "    'n_worker':16,\n",
    "    # ESM embedding\n",
    "    'epitope_max_len':128,\n",
    "    'antigen_max_len':64,\n",
    "    'epitope_batch_size':50,\n",
    "    'antigen_batch_size':100,\n",
    "    # Feature extraction\n",
    "    'CT_CTD_features' : False,\n",
    "    'CNT_features' : False,\n",
    "    'CT_CTD_PCA':0,\n",
    "    'CNT_PCA':0,\n",
    "    # Tabnet model\n",
    "    'epochs' : 100,\n",
    "    'patience' : 20,\n",
    "    'learning_rate':2e-2,\n",
    "    'weight_decay':1e-5,\n",
    "    'threshold':0.5,\n",
    "    'seed':42,\n",
    "    'fold':5\n",
    "}\n",
    "\n",
    "# seed setting function\n",
    "def seed_everything(seed:int):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(CONFIG['seed']) # Seed setting\n",
    "\n",
    "# tabnet params\n",
    "tabnet_params = dict(\n",
    "    n_d = 64,   # 8 to 64\n",
    "    n_a = 128,  # n_d = n_a usally good\n",
    "    n_steps = 1,\n",
    "    gamma = 1.3,\n",
    "    lambda_sparse = 0,\n",
    "    n_independent = 2,\n",
    "    n_shared = 1,\n",
    "    optimizer_fn = optim.Adam,\n",
    "    optimizer_params = dict(lr = CONFIG['learning_rate'], weight_decay = CONFIG['weight_decay']),\n",
    "    mask_type = \"entmax\",\n",
    "    scheduler_params = dict(\n",
    "        mode = \"min\", patience = 5, min_lr = 1e-5, factor = 0.9),\n",
    "    scheduler_fn = ReduceLROnPlateau,\n",
    "    seed = CONFIG[\"seed\"],\n",
    "    verbose = 5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "## ESM Embedding feature extraction\n",
    "\n",
    "- Using Left antigen(64), Epitope(128), Right antigen(64) sequences\n",
    "- 각 sequence 별 embedding 추출\n",
    "- 추출된 Embedding feature 파일이 있으면 생략됨. 재현 검증을 원할 시 ```Embeddings/``` 디렉토리 제거 후 실행\n",
    "- ESM : [ESM Github](https://github.com/facebookresearch/esm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def protein_sequence_preprocessing(new_df):\n",
    "    epitope_list = []\n",
    "    left_antigen_list = []\n",
    "    right_antigen_list = []\n",
    "    \n",
    "    for id_, epitope, antigen, s_p, e_p in tqdm(zip(new_df['id'],new_df['epitope_seq'], new_df['antigen_seq'], new_df['start_position'], new_df['end_position'])):\n",
    "        start_position = s_p-CONFIG['antigen_max_len']-1\n",
    "        end_position = e_p+CONFIG['antigen_max_len']\n",
    "        if start_position < 0:\n",
    "            start_position = 0\n",
    "        if end_position > len(antigen):\n",
    "            end_position = len(antigen)\n",
    "        \n",
    "        # left / right antigen sequence 추출\n",
    "        left_antigen = antigen[int(start_position) : int(s_p)-1]\n",
    "        right_antigen = antigen[int(e_p) : int(end_position)]\n",
    "\n",
    "        if CONFIG['epitope_max_len']<len(epitope):\n",
    "            epitope = epitope[:CONFIG['epitope_max_len']]\n",
    "\n",
    "        new_epitope = (id_, epitope)\n",
    "        new_left_antigen = (id_, left_antigen)\n",
    "        new_right_antigen = (id_, right_antigen)\n",
    "        \n",
    "        epitope_list.append(new_epitope)\n",
    "        left_antigen_list.append(new_left_antigen)\n",
    "        right_antigen_list.append(new_right_antigen)\n",
    "\n",
    "    return epitope_list, left_antigen_list, right_antigen_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset for ESM input\n",
    "- ESM 인코더에 입력하기 위한 Dataset 클래스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProteinDataset(Dataset):\n",
    "    def __init__(self, string_labels, protein_list):\n",
    "        self.string_labels = string_labels\n",
    "        self.protein_list = protein_list\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        self.string_label = self.string_labels[index]\n",
    "        self.protein = self.protein_list[index]\n",
    "\n",
    "        return self.string_label, torch.tensor(self.protein, dtype=torch.long)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.protein_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test sequence embedding extraction\n",
    "- Embedding Feature 추출하기 -> 빠른 실행을 위한 train, test set에 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efae3e34414348b5951929bba89c106a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe9d9c3ca2054d37a889b6fc42e3e857",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish data loading\n",
      "== train_epitope embedding ==\n",
      "== train_left embedding ==\n",
      "== train_right embedding ==\n",
      "== test_epitope embedding ==\n",
      "== test_left embedding ==\n",
      "== test_right embedding ==\n"
     ]
    }
   ],
   "source": [
    "# TRAIN_DATA_PATH = \"../dataset/train.csv\"\n",
    "# TEST_DATA_PATH = \"../dataset/test.csv\"\n",
    "TRAIN_DATA_PATH = \"/data/train.csv\"\n",
    "TEST_DATA_PATH = \"/data/test.csv\"\n",
    "\n",
    "SAVE_EMBEDDING_DIR = \"./Embeddings/\"\n",
    "if not os.path.exists(SAVE_EMBEDDING_DIR):\n",
    "    os.makedirs(SAVE_EMBEDDING_DIR)\n",
    "\n",
    "df_train = pd.read_csv(TRAIN_DATA_PATH)\n",
    "df_test = pd.read_csv(TEST_DATA_PATH)\n",
    "\n",
    "model, alphabet = esm.pretrained.esm1b_t33_650M_UR50S()\n",
    "batch_converter = alphabet.get_batch_converter()\n",
    "\n",
    "train_epitope_list, train_left_list, train_right_list = protein_sequence_preprocessing(df_train)\n",
    "test_epitope_list, test_left_list, test_right_list = protein_sequence_preprocessing(df_test)\n",
    "\n",
    "print(\"finish data loading\")\n",
    "\n",
    "params = {\n",
    "    \"train_epitope\":train_epitope_list,\n",
    "    \"train_left\":train_left_list,\n",
    "    \"train_right\":train_right_list,\n",
    "    \"test_epitope\":test_epitope_list,\n",
    "    \"test_left\":test_left_list,\n",
    "    \"test_right\":test_right_list\n",
    "}\n",
    "\n",
    "# load model\n",
    "model = model.eval()\n",
    "model = model.cuda()\n",
    "model = nn.DataParallel(model)      # we use 4 RTX3090 GPUs\n",
    "\n",
    "for task, protein_list in params.items():\n",
    "    print(f\"== {task} embedding ==\")\n",
    "    file_name = f\"{task}_embeddings.pkl\"\n",
    "    file_path = os.path.join(SAVE_EMBEDDING_DIR, file_name)\n",
    "    \n",
    "    if os.path.exists(file_path):\n",
    "        continue\n",
    "    \n",
    "    string_labels, _, batch_tokens = batch_converter(protein_list)\n",
    "    batch_tokens = batch_tokens.numpy()\n",
    "\n",
    "    dataset = ProteinDataset(string_labels, batch_tokens)\n",
    "    if task in [\"train_epitope\", \"test_epitope\"]:\n",
    "        dataloader = DataLoader(dataset, batch_size=CONFIG[\"epitope_batch_size\"], shuffle=False, num_workers=CONFIG['n_worker'])\n",
    "    else:\n",
    "        dataloader = DataLoader(dataset, batch_size=CONFIG[\"antigen_batch_size\"], shuffle=False, num_workers=CONFIG['n_worker'])\n",
    "\n",
    "    protein_embeddings = []\n",
    "    for idx, data in enumerate(tqdm(dataloader)):\n",
    "        s_labels, b_tokens = data\n",
    "        b_tokens = b_tokens.cuda()\n",
    "        with torch.no_grad():\n",
    "            embeddings = model(b_tokens, repr_layers=[33])\n",
    "        result = embeddings[\"representations\"][33]\n",
    "        result = result[:, 0, :]        # use BOS tokens\n",
    "        result = result.detach().cpu().numpy()\n",
    "        s_labels = s_labels.numpy()\n",
    "\n",
    "        for id_, emb_ in zip(s_labels, result):\n",
    "            protein_embeddings.append((id_, emb_))\n",
    "\n",
    "    del b_tokens, embeddings\n",
    "\n",
    "    # save files    \n",
    "    with open(file_path, \"wb\") as fw:\n",
    "        pickle.dump(protein_embeddings, fw)\n",
    "\n",
    "    print(f\"finish {task} embedding extraction\")\n",
    "    \n",
    "del model\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CT-CTD Feature extraction\n",
    "- 해당 fature 사용을 원할 시 config 파일의 ```CONFIG['CT_CTD_features']=True```로 변경\n",
    "- 이미 추출된 CT-CTD feature 파일이 있으면 생략됨\n",
    "- CT : Conjoint Triad features\n",
    "- CTD : Composition-Transition-Distribution features\n",
    "- Using 13 physicochemical attributes\n",
    "- paper : [CT-CTD feature paper](https://www.sciencedirect.com/science/article/pii/S0010482520302973)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CT-CTD feature extraction\n",
    "ct_group_dict = {\n",
    "    'A':1, 'G':1, 'V':1, 'C':2, 'D':3, 'E':3, 'F':4, 'I':4, 'L':4, 'P':4,\n",
    "    'H':5, 'N':5, 'Q':5, 'W':5, 'K':6, 'R':6, 'M':7, 'S':7, 'T':7, 'Y':7, \n",
    "}\n",
    "trid_groups = list(product(['1', '2', '3', '4', '5', '6', '7'], repeat=3))\n",
    "trid_dict = []\n",
    "for trid in trid_groups:\n",
    "    trid_dict.append(''.join(list(trid)))\n",
    "\n",
    "ctd_group_dict = [\n",
    "    ['RKEDQN', 'GASTPHY', 'CLVIMFW'],\n",
    "    ['QSTNGDE', 'RAHCKMV', 'LYPFIW'],\n",
    "    ['QNGSWTDERA', 'HMCKV', 'LPFYI'],\n",
    "    ['KPDESNQT', 'GRHA', 'YMFWLCVI'],\n",
    "    ['KDEQPSRNTG', 'AHYMLV', 'FIWC'],\n",
    "    ['RDKENQHYP', 'SGTAW', 'CVLIMF'],\n",
    "    ['KERSQD', 'NTPG', 'AYHWVMFLIC'],\n",
    "    ['GASCTPD', 'NVEQIL', 'MHKFRYW'],  \n",
    "    ['LIFWCMVY', 'PATGS', 'HQRKNED'],\n",
    "    ['GASDT', 'CPNVEQIL', 'KMHFRYW'],\n",
    "    ['KR', 'ANCQGHILMFPSTWYV', 'DE'],\n",
    "    ['EALMQKRH', 'VIYCWFT', 'GNPSD'],\n",
    "    ['ALFCGIVW', 'RKQEND', 'MPSTHY']\n",
    "]\n",
    "\n",
    "new_ctd_group_dict=[]\n",
    "\n",
    "for attribute in ctd_group_dict:\n",
    "    new_dict = dict()\n",
    "    for idx, att in enumerate(attribute):\n",
    "        for k in att:\n",
    "            new_dict[k]=idx\n",
    "    new_ctd_group_dict.append(new_dict)\n",
    "\n",
    "trans_dict={'01':0, '10':0, '12':1, '21':1, '02':2, '20':2}\n",
    "\n",
    "def get_CT_CTD_features(acid):\n",
    "    trid_features = []\n",
    "    # convert to group\n",
    "    grouped = ''.join(list(map(lambda x:str(ct_group_dict[x]), acid)))\n",
    "\n",
    "    # ====== extract CT\n",
    "    for trid in trid_dict:\n",
    "        trid_features.append(grouped.count(trid))\n",
    "\n",
    "    # ====== extract CTD\n",
    "    comp_features = []\n",
    "    trans_features = []\n",
    "    dist_features = []\n",
    "\n",
    "    for attribute in new_ctd_group_dict:\n",
    "        # ==== Compositon feature\n",
    "        comps_f = []\n",
    "        comps = ''.join(list(map(lambda x:str(attribute[x]), acid)))\n",
    "        try:\n",
    "            comps_f.append(comps.count('0')/len(acid))\n",
    "            comps_f.append(comps.count('1')/len(acid))\n",
    "            comps_f.append(comps.count('2')/len(acid))\n",
    "        except:\n",
    "            # division 0\n",
    "            comps_f=[0, 0, 0]\n",
    "        \n",
    "        # ==== Transition feature\n",
    "        trans_f = [0, 0, 0]\n",
    "        for idx in range(len(comps)-1):\n",
    "            try:\n",
    "                trans = comps[idx:idx+2]\n",
    "                trans_f[trans_dict[trans]]+=1\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        if len(comps)==0:\n",
    "            trans_f = [0, 0, 0]\n",
    "        else:\n",
    "            try:\n",
    "                trans_f[0]/=len(comps)-1\n",
    "                trans_f[1]/=len(comps)-1\n",
    "                trans_f[2]/=len(comps)-1\n",
    "            except:\n",
    "                trans_f = [0, 0, 0]\n",
    "\n",
    "        # ==== Distribution feature\n",
    "        dist_f = []\n",
    "        np_comps = np.array(list(comps))\n",
    "\n",
    "        for pos in ['0', '1', '2']:\n",
    "            g_pos = np.where(np_comps==pos)[0]\n",
    "            try:\n",
    "                g_pos -=g_pos[0]\n",
    "            except:\n",
    "                dist_f.extend([0, 0, 0, 0, 0])\n",
    "                continue\n",
    "            g_pos += 1\n",
    "            g_pos = g_pos.tolist()\n",
    "            pos_len = len(g_pos)\n",
    "            \n",
    "            dist_f.append(1/len(comps))\n",
    "            dist_f.append(g_pos[int(0.25*pos_len)-1]/len(comps))\n",
    "            dist_f.append(g_pos[int(0.5*pos_len)-1]/len(comps))\n",
    "            dist_f.append(g_pos[int(0.75*pos_len)-1]/len(comps))\n",
    "            dist_f.append(g_pos[int(pos_len)-1]/len(comps))\n",
    "        \n",
    "\n",
    "        comp_features.append(comps_f)\n",
    "        trans_features.append(trans_f)\n",
    "        dist_features.append(dist_f)\n",
    "\n",
    "\n",
    "    return trid_features, comp_features, trans_features, dist_features\n",
    "\n",
    "def extract_ct_ctd_features(df):\n",
    "    result_features = []\n",
    "    \n",
    "    for epitope, antigen in tqdm(zip(df['epitope_seq'], df['antigen_seq'])):\n",
    "        length_features = [len(epitope), len(antigen)]\n",
    "\n",
    "        # CT, CTD features\n",
    "        epit_ct, epit_comp, epit_trans, epit_dist = get_CT_CTD_features(epitope)\n",
    "        anti_ct, anti_comp, anti_trans, anti_dist = get_CT_CTD_features(antigen)\n",
    "\n",
    "        ct_ctd_features = []\n",
    "\n",
    "        ct_ctd_features+= epit_ct\n",
    "        for arr in [epit_comp, epit_trans, epit_dist]:\n",
    "            ct_ctd_features += np.array(arr).reshape(-1).tolist()\n",
    "\n",
    "        ct_ctd_features+= anti_ct\n",
    "        for arr in [anti_comp, anti_trans, anti_dist]:\n",
    "            ct_ctd_features += np.array(arr).reshape(-1).tolist()\n",
    "\n",
    "        extracted_features = length_features + ct_ctd_features\n",
    "        result_features.append(extracted_features)\n",
    "    \n",
    "    result_features = np.array(result_features)\n",
    "\n",
    "    return result_features\n",
    "\n",
    "\n",
    "TRAIN_CT_CTD_PATH = f\"train_epitope_antigen_CTCTD.pkl\"\n",
    "TEST_CT_CTD_PATH = f\"test_epitope_antigen_CTCTD.pkl\"\n",
    "CT_CTD_DIR = \"CT_CTD_Features\"\n",
    "\n",
    "if CONFIG['CT_CTD_features']:\n",
    "    if not os.path.exists(CT_CTD_DIR):\n",
    "        os.makedirs(CT_CTD_DIR)\n",
    "\n",
    "    train_ct_ctd_path = os.path.join(CT_CTD_DIR, TRAIN_CT_CTD_PATH)\n",
    "    test_ct_ctd_path = os.path.join(CT_CTD_DIR, TEST_CT_CTD_PATH)\n",
    "\n",
    "    if os.path.exists(train_ct_ctd_path) and os.path.exists(test_ct_ctd_path):\n",
    "        pass\n",
    "    else:\n",
    "        train_ct_ctd_features = extract_ct_ctd_features(df_train)\n",
    "        test_ct_ctd_features = extract_ct_ctd_features(df_test)\n",
    "\n",
    "        # Save\n",
    "        with open(train_ct_ctd_path, \"wb\") as fw:\n",
    "            pickle.dump(train_ct_ctd_features, fw)\n",
    "\n",
    "        with open(test_ct_ctd_path, \"wb\") as fw:\n",
    "            pickle.dump(test_ct_ctd_features, fw)\n",
    "            \n",
    "        del train_ct_ctd_features\n",
    "        del test_ct_ctd_features\n",
    "\n",
    "        print(f\"finished extract CT-CTD features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting features : Amino acids / Dipeptides\n",
    "- 해당 fature 사용을 원할 시 config 파일의 ```CONFIG['CNT_features']=True```로 변경\n",
    "- 이미 추출된 CNT feature 파일이 있으면 생략됨\n",
    "- Frequency feature of amino acids and dipeptidse in protein sequences\n",
    "- Reference : [FEGS](https://bmcbioinformatics.biomedcentral.com/track/pdf/10.1186/s12859-021-04223-3.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CNT feature extraction\n",
    "protein_list = ['A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y']\n",
    "combs_protein_list = []\n",
    "\n",
    "seques =  list(product(protein_list, repeat=2))\n",
    "for comb in seques:\n",
    "    combs_protein_list.append(''.join(list(comb)))\n",
    "\n",
    "\n",
    "def get_counting_features(counting_dicts, protein_seq):\n",
    "    single_dict, double_dict = counting_dicts\n",
    "    left_antigen, epitope, right_antigen = protein_seq\n",
    "\n",
    "    left_single, epitope_single, right_single = [], [], []\n",
    "    left_double, epitope_double, right_double = [], [], []\n",
    "\n",
    "    for single_acid in single_dict:\n",
    "        left_single.append(left_antigen.count(single_acid)) \n",
    "        epitope_single.append(epitope.count(single_acid))\n",
    "        right_single.append(right_antigen.count(single_acid)) \n",
    "\n",
    "    for double_acid in double_dict:\n",
    "        left_double.append(left_antigen.count(double_acid)) \n",
    "        epitope_double.append(epitope.count(double_acid))\n",
    "        right_double.append(right_antigen.count(double_acid)) \n",
    "\n",
    "    counting_features = left_single + epitope_single + right_single + left_double + epitope_double + right_double\n",
    "\n",
    "    return counting_features\n",
    "\n",
    "def extract_cnt_features(df):\n",
    "    result_features = []\n",
    "    \n",
    "    for epitope, antigen, s_p, e_p in tqdm(zip(df['epitope_seq'], df['antigen_seq'], df['start_position'], df['end_position'])):\n",
    "        left_antigen = antigen[: int(s_p)-1]\n",
    "        right_antigen = antigen[int(e_p) : ]\n",
    "\n",
    "        # counting features\n",
    "        counting_features = get_counting_features((protein_list, combs_protein_list), (left_antigen, epitope, right_antigen))\n",
    "        result_features.append(counting_features)\n",
    "    \n",
    "    result_features = np.array(result_features)\n",
    "\n",
    "    return result_features\n",
    "\n",
    "\n",
    "TRAIN_CNT_PATH = f\"train_epitope_antigen_CNT.pkl\"\n",
    "TEST_CNT_PATH = f\"test_epitope_antigen_CNT.pkl\"\n",
    "CNT_DIR = \"CNT_Features\"\n",
    "\n",
    "\n",
    "if CONFIG['CNT_features']:\n",
    "\n",
    "    if not os.path.exists(CNT_DIR):\n",
    "        os.makedirs(CNT_DIR)\n",
    "\n",
    "    train_cnt_path = os.path.join(CNT_DIR, TRAIN_CNT_PATH)\n",
    "    test_cnt_path = os.path.join(CNT_DIR, TEST_CNT_PATH)\n",
    "\n",
    "    if os.path.exists(train_cnt_path) and os.path.exists(test_cnt_path):\n",
    "        pass\n",
    "    else:\n",
    "        train_cnt_features = extract_cnt_features(df_train)\n",
    "        test_cnt_features = extract_cnt_features(df_test)\n",
    "        # Save\n",
    "        with open(train_cnt_path, \"wb\") as fw:\n",
    "            pickle.dump(train_cnt_features, fw)\n",
    "\n",
    "        with open(test_cnt_path, \"wb\") as fw:\n",
    "            pickle.dump(test_cnt_features, fw)\n",
    "\n",
    "        del train_cnt_features\n",
    "        del test_cnt_features\n",
    "\n",
    "        print(f\"finished extract CNT features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Norm and PCA for dimension reduction\n",
    "- 추출된 feature의 normalization과 dimension reduction을 위한 함수\n",
    "- scaler와 pca가 pickle형태로 저장됨. test 시 해당 파일 불러와서 적용 가능\n",
    "- Tabnet의 평가지표로 F1-macro로 사용하기 위한 커스텀 메트릭 클래스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============= Norm \n",
    "def norm_transform(datatype, data, scaler_name='z-score', scaler=None):\n",
    "    scaler_dict = {\n",
    "        'z-score':StandardScaler(),\n",
    "        'minmax':MinMaxScaler(),\n",
    "        'maxabs':MaxAbsScaler(),\n",
    "        'robust':RobustScaler(),\n",
    "        'norm':Normalizer()\n",
    "    }\n",
    "    \n",
    "    # use only train\n",
    "    if not datatype==\"test\":\n",
    "        scaler = scaler_dict[scaler_name]\n",
    "        scaled_train = scaler.fit_transform(data)\n",
    "        return scaled_train, scaler\n",
    "    else:\n",
    "        scaled_test = scaler.transform(data)\n",
    "        return scaled_test\n",
    "\n",
    "# ============= pca \n",
    "def pca_transform(datatype, data, n_comp=300, pca=None):\n",
    "    if not datatype==\"test\":\n",
    "        pca = PCA(n_components=n_comp, random_state=CONFIG[\"seed\"])\n",
    "        pca_train = pca.fit_transform(data)\n",
    "        print(f\"with {n_comp} components, pca variance ratio : {sum(pca.explained_variance_ratio_)}\")\n",
    "        return pca_train, pca\n",
    "    else:\n",
    "        pca_test = pca.transform(data)\n",
    "        return pca_test\n",
    "\n",
    "class F1_macro(Metric):\n",
    "    def __init__(self):\n",
    "        self._name = \"f1_macro\"\n",
    "        self._maximize = True\n",
    "\n",
    "    def __call__(self, y_true, y_score):\n",
    "        y_score = np.argmax(y_score, axis=1)\n",
    "        f1 = f1_score(y_true, y_score, average=\"macro\")\n",
    "        return f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Traning (Tabnet)\n",
    "-  Tabnet model : [Tabnet Github](https://github.com/dreamquark-ai/tabnet)\n",
    "-  Ebmedding feature preprocessing 함수\n",
    "-  left antigen, epitope, right antigen embedding feature와 category feature를 합쳐서 출력함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for tabnet data\n",
    "def get_preprocessing(data_type, epitope_data, left_anti, right_anti, new_df):\n",
    "    tab_net_feature_list = []\n",
    "\n",
    "    features_cols = [\"assay_method_technique\", \"assay_group\", \"disease_type\", \"disease_state\", \"reference_date\", \"reference_journal\", \"reference_title\"]\n",
    "    feature_data = enc.transform(new_df[features_cols]).toarray()\n",
    "\n",
    "    # sort by id\n",
    "    epitope_data.sort()\n",
    "    left_anti.sort()\n",
    "    right_anti.sort()\n",
    "    \n",
    "    for df_id, epi_embs, left_embs, right_embs, feature in tqdm(zip(new_df['id'], epitope_data, left_anti, right_anti, feature_data)):\n",
    "        if not df_id == epi_embs[0]:\n",
    "            print(\"Not matched ID\")\n",
    "        tab_net_features = np.append(epi_embs[1], left_embs[1])\n",
    "        tab_net_features = np.append(tab_net_features , right_embs[1])\n",
    "        tab_net_features = np.append(tab_net_features , feature)\n",
    "        tab_net_feature_list.append(tab_net_features)\n",
    "    \n",
    "    label_list = None\n",
    "    if data_type != 'test':\n",
    "        label_list = []\n",
    "        for label in new_df['label']:\n",
    "            label_list.append(label)\n",
    "        label_list = np.array(label_list)\n",
    "    print(f'{data_type} dataframe preprocessing was done.')\n",
    "\n",
    "    tab_net_feature_list = np.array(tab_net_feature_list)\n",
    "    \n",
    "    return tab_net_feature_list, label_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "- Using ESM embedding of **left antigen, epitope, right antigen**\n",
    "- Categrical features to one-hot encoding\n",
    "- if you want to use other features, set ```CONFIG['CT_CTD_features']=True``` or ```CONFIG['CNT_features']=True```\n",
    "- if you want to use PCA for features, set ```CONFIG['CT_CTC_PCA]``` or ```CONFIG['CNT_PCA]```\n",
    "\n",
    "#### 5-fold for validation and use mean of prediction logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ede503b85fd640d3bd7f17c69e37e663",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataframe preprocessing was done.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "733f9bd44e6c4c56bbbf071bae073325",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test dataframe preprocessing was done.\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.21128 | val_0_auc: 0.9059  | val_0_accuracy: 0.91282 | val_0_f1_macro: 0.48019 |  0:00:05s\n",
      "epoch 5  | loss: 0.12062 | val_0_auc: 0.82127 | val_0_accuracy: 0.91667 | val_0_f1_macro: 0.60592 |  0:00:33s\n",
      "epoch 10 | loss: 0.10866 | val_0_auc: 0.80546 | val_0_accuracy: 0.91612 | val_0_f1_macro: 0.59483 |  0:01:00s\n",
      "epoch 15 | loss: 0.09735 | val_0_auc: 0.97191 | val_0_accuracy: 0.95632 | val_0_f1_macro: 0.84935 |  0:01:28s\n",
      "epoch 20 | loss: 0.08983 | val_0_auc: 0.97191 | val_0_accuracy: 0.9554  | val_0_f1_macro: 0.84411 |  0:01:55s\n",
      "epoch 25 | loss: 0.08432 | val_0_auc: 0.96988 | val_0_accuracy: 0.95556 | val_0_f1_macro: 0.84927 |  0:02:23s\n",
      "epoch 30 | loss: 0.08105 | val_0_auc: 0.97311 | val_0_accuracy: 0.95629 | val_0_f1_macro: 0.8529  |  0:02:50s\n",
      "epoch 35 | loss: 0.0765  | val_0_auc: 0.97262 | val_0_accuracy: 0.95697 | val_0_f1_macro: 0.85816 |  0:03:18s\n",
      "epoch 40 | loss: 0.06908 | val_0_auc: 0.97087 | val_0_accuracy: 0.95532 | val_0_f1_macro: 0.85642 |  0:03:45s\n",
      "epoch 45 | loss: 0.06659 | val_0_auc: 0.96962 | val_0_accuracy: 0.95624 | val_0_f1_macro: 0.85883 |  0:04:13s\n",
      "epoch 50 | loss: 0.0591  | val_0_auc: 0.96919 | val_0_accuracy: 0.95671 | val_0_f1_macro: 0.85856 |  0:04:41s\n",
      "epoch 55 | loss: 0.05938 | val_0_auc: 0.97005 | val_0_accuracy: 0.95676 | val_0_f1_macro: 0.85873 |  0:05:09s\n",
      "epoch 60 | loss: 0.05593 | val_0_auc: 0.97014 | val_0_accuracy: 0.95522 | val_0_f1_macro: 0.8586  |  0:05:37s\n",
      "epoch 65 | loss: 0.0521  | val_0_auc: 0.96845 | val_0_accuracy: 0.95676 | val_0_f1_macro: 0.85731 |  0:06:04s\n",
      "\n",
      "Early stopping occurred at epoch 69 with best_epoch = 49 and best_val_0_f1_macro = 0.86118\n",
      "Best weights from best epoch are automatically used!\n",
      "Successfully saved model at ./Tabnet_ESM_models/./tabnet_esm_model_fold1.zip\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.2046  | val_0_auc: 0.89531 | val_0_accuracy: 0.90954 | val_0_f1_macro: 0.4766  |  0:00:05s\n",
      "epoch 5  | loss: 0.1182  | val_0_auc: 0.95718 | val_0_accuracy: 0.94867 | val_0_f1_macro: 0.81835 |  0:00:33s\n",
      "epoch 10 | loss: 0.10441 | val_0_auc: 0.96829 | val_0_accuracy: 0.95165 | val_0_f1_macro: 0.83125 |  0:01:00s\n",
      "epoch 15 | loss: 0.09281 | val_0_auc: 0.97068 | val_0_accuracy: 0.95268 | val_0_f1_macro: 0.85241 |  0:01:28s\n",
      "epoch 20 | loss: 0.08716 | val_0_auc: 0.94798 | val_0_accuracy: 0.94196 | val_0_f1_macro: 0.78703 |  0:01:56s\n",
      "epoch 25 | loss: 0.07879 | val_0_auc: 0.96925 | val_0_accuracy: 0.95372 | val_0_f1_macro: 0.85167 |  0:02:23s\n",
      "epoch 30 | loss: 0.07638 | val_0_auc: 0.96888 | val_0_accuracy: 0.9532  | val_0_f1_macro: 0.84778 |  0:02:50s\n",
      "epoch 35 | loss: 0.07074 | val_0_auc: 0.96764 | val_0_accuracy: 0.95383 | val_0_f1_macro: 0.85059 |  0:03:18s\n",
      "epoch 40 | loss: 0.06833 | val_0_auc: 0.96615 | val_0_accuracy: 0.95519 | val_0_f1_macro: 0.85203 |  0:03:46s\n",
      "epoch 45 | loss: 0.0606  | val_0_auc: 0.96674 | val_0_accuracy: 0.95359 | val_0_f1_macro: 0.85439 |  0:04:13s\n",
      "\n",
      "Early stopping occurred at epoch 49 with best_epoch = 29 and best_val_0_f1_macro = 0.85617\n",
      "Best weights from best epoch are automatically used!\n",
      "Successfully saved model at ./Tabnet_ESM_models/./tabnet_esm_model_fold2.zip\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.21262 | val_0_auc: 0.90467 | val_0_accuracy: 0.91156 | val_0_f1_macro: 0.47687 |  0:00:05s\n",
      "epoch 5  | loss: 0.12093 | val_0_auc: 0.9584  | val_0_accuracy: 0.94542 | val_0_f1_macro: 0.77837 |  0:00:32s\n",
      "epoch 10 | loss: 0.10739 | val_0_auc: 0.97005 | val_0_accuracy: 0.95304 | val_0_f1_macro: 0.84644 |  0:01:00s\n",
      "epoch 15 | loss: 0.0973  | val_0_auc: 0.97196 | val_0_accuracy: 0.95391 | val_0_f1_macro: 0.84355 |  0:01:28s\n",
      "epoch 20 | loss: 0.08918 | val_0_auc: 0.97355 | val_0_accuracy: 0.95603 | val_0_f1_macro: 0.85306 |  0:01:55s\n",
      "epoch 25 | loss: 0.08313 | val_0_auc: 0.97172 | val_0_accuracy: 0.95427 | val_0_f1_macro: 0.84753 |  0:02:23s\n",
      "epoch 30 | loss: 0.07882 | val_0_auc: 0.96997 | val_0_accuracy: 0.95435 | val_0_f1_macro: 0.84541 |  0:02:50s\n",
      "epoch 35 | loss: 0.07292 | val_0_auc: 0.9678  | val_0_accuracy: 0.95535 | val_0_f1_macro: 0.84588 |  0:03:18s\n",
      "epoch 40 | loss: 0.0672  | val_0_auc: 0.96813 | val_0_accuracy: 0.95443 | val_0_f1_macro: 0.85079 |  0:03:46s\n",
      "epoch 45 | loss: 0.06764 | val_0_auc: 0.96909 | val_0_accuracy: 0.95551 | val_0_f1_macro: 0.85683 |  0:04:13s\n",
      "epoch 50 | loss: 0.05784 | val_0_auc: 0.96336 | val_0_accuracy: 0.95383 | val_0_f1_macro: 0.8513  |  0:04:41s\n",
      "epoch 55 | loss: 0.0562  | val_0_auc: 0.96677 | val_0_accuracy: 0.95346 | val_0_f1_macro: 0.84725 |  0:05:08s\n",
      "epoch 60 | loss: 0.05335 | val_0_auc: 0.9626  | val_0_accuracy: 0.95422 | val_0_f1_macro: 0.85138 |  0:05:36s\n",
      "epoch 65 | loss: 0.05065 | val_0_auc: 0.96541 | val_0_accuracy: 0.95325 | val_0_f1_macro: 0.84877 |  0:06:04s\n",
      "\n",
      "Early stopping occurred at epoch 65 with best_epoch = 45 and best_val_0_f1_macro = 0.85683\n",
      "Best weights from best epoch are automatically used!\n",
      "Successfully saved model at ./Tabnet_ESM_models/./tabnet_esm_model_fold3.zip\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.21264 | val_0_auc: 0.90129 | val_0_accuracy: 0.91452 | val_0_f1_macro: 0.4789  |  0:00:05s\n",
      "epoch 5  | loss: 0.12032 | val_0_auc: 0.96251 | val_0_accuracy: 0.95037 | val_0_f1_macro: 0.83341 |  0:00:33s\n",
      "epoch 10 | loss: 0.10932 | val_0_auc: 0.96929 | val_0_accuracy: 0.95446 | val_0_f1_macro: 0.84704 |  0:01:01s\n",
      "epoch 15 | loss: 0.10272 | val_0_auc: 0.97028 | val_0_accuracy: 0.95514 | val_0_f1_macro: 0.84207 |  0:01:29s\n",
      "epoch 20 | loss: 0.0975  | val_0_auc: 0.97003 | val_0_accuracy: 0.95627 | val_0_f1_macro: 0.84985 |  0:01:57s\n",
      "epoch 25 | loss: 0.09395 | val_0_auc: 0.9705  | val_0_accuracy: 0.95524 | val_0_f1_macro: 0.8499  |  0:02:24s\n",
      "epoch 30 | loss: 0.08604 | val_0_auc: 0.96512 | val_0_accuracy: 0.95315 | val_0_f1_macro: 0.84004 |  0:02:52s\n",
      "epoch 35 | loss: 0.08315 | val_0_auc: 0.96973 | val_0_accuracy: 0.95663 | val_0_f1_macro: 0.8483  |  0:03:20s\n",
      "epoch 40 | loss: 0.07732 | val_0_auc: 0.96983 | val_0_accuracy: 0.956   | val_0_f1_macro: 0.84845 |  0:03:48s\n",
      "epoch 45 | loss: 0.07313 | val_0_auc: 0.97016 | val_0_accuracy: 0.95522 | val_0_f1_macro: 0.856   |  0:04:16s\n",
      "epoch 50 | loss: 0.07106 | val_0_auc: 0.96945 | val_0_accuracy: 0.95729 | val_0_f1_macro: 0.85747 |  0:04:43s\n",
      "epoch 55 | loss: 0.06623 | val_0_auc: 0.96792 | val_0_accuracy: 0.95606 | val_0_f1_macro: 0.85591 |  0:05:11s\n",
      "epoch 60 | loss: 0.06326 | val_0_auc: 0.96636 | val_0_accuracy: 0.95679 | val_0_f1_macro: 0.85303 |  0:05:38s\n",
      "epoch 65 | loss: 0.06113 | val_0_auc: 0.96505 | val_0_accuracy: 0.9559  | val_0_f1_macro: 0.85129 |  0:06:06s\n",
      "epoch 70 | loss: 0.06242 | val_0_auc: 0.96846 | val_0_accuracy: 0.95674 | val_0_f1_macro: 0.85536 |  0:06:34s\n",
      "\n",
      "Early stopping occurred at epoch 70 with best_epoch = 50 and best_val_0_f1_macro = 0.85747\n",
      "Best weights from best epoch are automatically used!\n",
      "Successfully saved model at ./Tabnet_ESM_models/./tabnet_esm_model_fold4.zip\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.21493 | val_0_auc: 0.88698 | val_0_accuracy: 0.91028 | val_0_f1_macro: 0.47652 |  0:00:05s\n",
      "epoch 5  | loss: 0.11969 | val_0_auc: 0.9514  | val_0_accuracy: 0.94819 | val_0_f1_macro: 0.81616 |  0:00:33s\n",
      "epoch 10 | loss: 0.10226 | val_0_auc: 0.96507 | val_0_accuracy: 0.9494  | val_0_f1_macro: 0.82506 |  0:01:00s\n",
      "epoch 15 | loss: 0.09349 | val_0_auc: 0.96167 | val_0_accuracy: 0.94783 | val_0_f1_macro: 0.81492 |  0:01:28s\n",
      "epoch 20 | loss: 0.08687 | val_0_auc: 0.96876 | val_0_accuracy: 0.95092 | val_0_f1_macro: 0.82922 |  0:01:55s\n",
      "epoch 25 | loss: 0.08106 | val_0_auc: 0.9675  | val_0_accuracy: 0.95244 | val_0_f1_macro: 0.83789 |  0:02:23s\n",
      "epoch 30 | loss: 0.07896 | val_0_auc: 0.96546 | val_0_accuracy: 0.95105 | val_0_f1_macro: 0.8397  |  0:02:50s\n",
      "epoch 35 | loss: 0.07677 | val_0_auc: 0.96679 | val_0_accuracy: 0.95137 | val_0_f1_macro: 0.84253 |  0:03:18s\n",
      "epoch 40 | loss: 0.06894 | val_0_auc: 0.96485 | val_0_accuracy: 0.95207 | val_0_f1_macro: 0.83848 |  0:03:46s\n",
      "epoch 45 | loss: 0.06451 | val_0_auc: 0.96476 | val_0_accuracy: 0.9516  | val_0_f1_macro: 0.84519 |  0:04:13s\n",
      "epoch 50 | loss: 0.0609  | val_0_auc: 0.96324 | val_0_accuracy: 0.95205 | val_0_f1_macro: 0.84328 |  0:04:41s\n",
      "epoch 55 | loss: 0.05913 | val_0_auc: 0.96357 | val_0_accuracy: 0.95299 | val_0_f1_macro: 0.84526 |  0:05:09s\n",
      "\n",
      "Early stopping occurred at epoch 59 with best_epoch = 39 and best_val_0_f1_macro = 0.84863\n",
      "Best weights from best epoch are automatically used!\n",
      "Successfully saved model at ./Tabnet_ESM_models/./tabnet_esm_model_fold5.zip\n"
     ]
    }
   ],
   "source": [
    "# PATH\n",
    "EPITOPE_EMB_PATH = \"./Embeddings/train_epitope_embeddings.pkl\"\n",
    "LEFT_EMB_PATH = \"./Embeddings/train_left_embeddings.pkl\"\n",
    "RIGHT_EMB_PATH = \"./Embeddings/train_right_embeddings.pkl\"\n",
    "TEST_EPITOPE_EMB_PATH = \"./Embeddings/test_epitope_embeddings.pkl\"\n",
    "TEST_LEFT_EMB_PATH = \"./Embeddings/test_left_embeddings.pkl\"\n",
    "TEST_RIGHT_EMB_PATH = \"./Embeddings/test_right_embeddings.pkl\"\n",
    "\n",
    "\n",
    "MODEL_DIR_NAME = \"./Tabnet_ESM_models\"\n",
    "\n",
    "if not os.path.exists(MODEL_DIR_NAME):\n",
    "    os.makedirs(MODEL_DIR_NAME)\n",
    "    \n",
    "df_train = pd.read_csv(TRAIN_DATA_PATH)\n",
    "df_test = pd.read_csv(TEST_DATA_PATH)\n",
    "\n",
    "\n",
    "# Load extraction features\n",
    "\n",
    "# load ESM embeddings  [ 1280 x 3 ] : [ 3840 features]\n",
    "with open(EPITOPE_EMB_PATH, \"rb\") as fr:\n",
    "    epitope_data = pickle.load(fr)\n",
    "with open(LEFT_EMB_PATH, \"rb\") as fr:\n",
    "    left_anti_data = pickle.load(fr)\n",
    "with open(RIGHT_EMB_PATH, \"rb\") as fr:\n",
    "    right_anti_data = pickle.load(fr)\n",
    "\n",
    "# Test\n",
    "with open(TEST_EPITOPE_EMB_PATH, \"rb\") as fr:\n",
    "    test_epitope_data = pickle.load(fr)\n",
    "with open(TEST_LEFT_EMB_PATH, \"rb\") as fr:\n",
    "    test_left_anti_data = pickle.load(fr)\n",
    "with open(TEST_RIGHT_EMB_PATH, \"rb\") as fr:\n",
    "    test_right_anti_data = pickle.load(fr)\n",
    "\n",
    "\n",
    "\n",
    "# label one hot encoding\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "features_cols = [\"assay_method_technique\", \"assay_group\", \"disease_type\", \"disease_state\", \"reference_date\", \"reference_journal\", \"reference_title\"]\n",
    "enc.fit(df_train[features_cols])\n",
    "\n",
    "tab_net_feature_list, label_list = get_preprocessing('train', epitope_data, left_anti_data, right_anti_data, df_train)\n",
    "test_tab_net_feature_list, test_label_list = get_preprocessing('test', test_epitope_data, test_left_anti_data, test_right_anti_data, df_test)\n",
    "\n",
    "\n",
    "# CT-CTD features [ 3 + (616 x 3) ] : [1851 features]\n",
    "if CONFIG['CT_CTD_features']:\n",
    "    TRAIN_CT_CTD_PATH = f\"train_epitope_antigen_CTCTD.pkl\"\n",
    "    TEST_CT_CTD_PATH = f\"test_epitope_antigen_CTCTD.pkl\"\n",
    "    CT_CTD_SCALER = f\"CTCTD_scaler.pkl\"\n",
    "    CT_CTD_PCA = f\"CTCTD_pca.pkl\"\n",
    "    CT_CTD_DIR = \"CT_CTD_Features\"\n",
    "\n",
    "    train_ct_ctd_path = os.path.join(CT_CTD_DIR, TRAIN_CT_CTD_PATH)\n",
    "    test_ct_ctd_path = os.path.join(CT_CTD_DIR, TEST_CT_CTD_PATH)\n",
    "    scaler_ct_ctd_path = os.path.join(CT_CTD_DIR, CT_CTD_SCALER)\n",
    "    pca_ct_ctd__path = os.path.join(CT_CTD_DIR, CT_CTD_PCA)\n",
    "\n",
    "    with open(train_ct_ctd_path, \"rb\") as fr:\n",
    "        train_ct_ctd_features = pickle.load(fr)\n",
    "\n",
    "    with open(test_ct_ctd_path, \"rb\") as fr:\n",
    "        test_ct_ctd_features= pickle.load(fr)\n",
    "\n",
    "    # Norm\n",
    "    train_ct_ctd_features, scaler = norm_transform(\"train\", train_ct_ctd_features, \"minmax\")\n",
    "    test_ct_ctd_features = norm_transform(\"test\", test_ct_ctd_features, \"minmax\", scaler)\n",
    "    # need to save norm scaler\n",
    "    print(f\"CT-CTD feature normalization finished\")\n",
    "\n",
    "    with open(scaler_ct_ctd_path, \"wb\") as fw:\n",
    "        pickle.dump(scaler, fw)\n",
    "\n",
    "    # PCA\n",
    "    if not CONFIG['CT_CTD_PCA']==0:\n",
    "        n_comps = CONFIG['CT_CTD_PCA']\n",
    "        before_features = train_ct_ctd_features.shape[1]\n",
    "        train_ct_ctd_features, pca = pca_transform(\"train\", train_ct_ctd_features, n_comps)\n",
    "        test_ct_ctd_features = pca_transform(\"test\", test_ct_ctd_features, n_comps, pca)\n",
    "        # need to save pca scaler\n",
    "        print(f\"CT_CTD feature decomposition finished {before_features} -> {n_comps}\")\n",
    "\n",
    "        with open(pca_ct_ctd__path, \"wb\") as fw:\n",
    "            pickle.dump(pca, fw)\n",
    "\n",
    "    tab_net_feature_list = np.concatenate((train_ct_ctd_features, tab_net_feature_list), axis=1)\n",
    "    test_tab_net_feature_list = np.concatenate((test_ct_ctd_features, test_tab_net_feature_list), axis=1)\n",
    "\n",
    "\n",
    "# CNT features [ 420 x 3 ] : [1260 features]\n",
    "if CONFIG['CNT_features']:\n",
    "    TRAIN_CNT_PATH = f\"train_epitope_antigen_CNT.pkl\"\n",
    "    TEST_CNT_PATH = f\"test_epitope_antigen_CNT.pkl\"\n",
    "    CNT_SCALER = f\"CNT_scaler.pkl\"\n",
    "    CNT_PCA = f\"CNT_pca.pkl\"\n",
    "    CNT_DIR = \"CNT_Features\"\n",
    "    train_cnt_path = os.path.join(CNT_DIR, TRAIN_CNT_PATH)\n",
    "    test_cnt_path = os.path.join(CNT_DIR, TEST_CNT_PATH)\n",
    "    scaler_cnt_path = os.path.join(CNT_DIR, CNT_SCALER)\n",
    "    pca_cnt_path = os.path.join(CNT_DIR, CNT_PCA)\n",
    "    \n",
    "    with open(train_cnt_path, \"rb\") as fr:\n",
    "        train_cnt_features= pickle.load(fr)\n",
    "\n",
    "    with open(test_cnt_path, \"rb\") as fr:\n",
    "        test_cnt_features= pickle.load(fr)\n",
    "    # Norm\n",
    "    train_cnt_features, scaler = norm_transform(\"train\", train_cnt_features, \"minmax\")\n",
    "    test_cnt_features = norm_transform(\"test\", test_cnt_features, \"minmax\", scaler)\n",
    "    # need to save norm scaler\n",
    "\n",
    "    print(f\"CNT feature normalization finished\")\n",
    "    with open(scaler_cnt_path, \"wb\") as fw:\n",
    "        pickle.dump(scaler, fw)\n",
    "\n",
    "\n",
    "    # PCA\n",
    "    if not CONFIG['CNT_PCA']==0:\n",
    "        n_comps = CONFIG['CNT_PCA']\n",
    "        before_features = train_cnt_features.shape[1]\n",
    "        train_cnt_features, pca = pca_transform(\"train\", train_cnt_features, n_comps)\n",
    "        test_cnt_features = pca_transform(\"test\", test_cnt_features, n_comps, pca)\n",
    "        # need to save pca scaler\n",
    "        print(f\"CNT feature decomposition finished {before_features.shape[1]} -> {n_comps}\")\n",
    "        \n",
    "        with open(pca_cnt_path, \"wb\") as fw:\n",
    "            pickle.dump(pca, fw)\n",
    "\n",
    "    tab_net_feature_list = np.concatenate((train_cnt_features, tab_net_feature_list), axis=1)\n",
    "    test_tab_net_feature_list = np.concatenate((test_cnt_features, test_tab_net_feature_list), axis=1)\n",
    "\n",
    "\n",
    "kf = KFold(n_splits=CONFIG['fold'],  random_state=CONFIG['seed'], shuffle=True)\n",
    "avg_loss, avg_f1 = 0, 0\n",
    "\n",
    "LOG_PATH= os.path.join(MODEL_DIR_NAME,\"log.txt\")\n",
    "\n",
    "with open(LOG_PATH, \"w\") as fw:\n",
    "    fw.write(\"========================================\\n\")\n",
    "    fw.write(\"=   Tabnet model with ESM embeddings   =\\n\")\n",
    "    fw.write(\"=--------------------------------------=\\n\")\n",
    "    fw.write(\"=        128 epitope embeddings        =\\n\")\n",
    "    fw.write(\"=      64 left antigen embeddings      =\\n\")\n",
    "    fw.write(\"=      64 right antigen embeddings     =\\n\")\n",
    "    fw.write(\"========================================\\n\\n\")\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(kf.split(tab_net_feature_list)):\n",
    "        fw.write(f\": ========= FOLD {fold+1} ========= :\\n\")\n",
    "\n",
    "        model = TabNetClassifier(**tabnet_params)\n",
    "        model.fit(X_train=tab_net_feature_list[train_idx], y_train=label_list[train_idx],\n",
    "                    eval_set=[(tab_net_feature_list[test_idx], label_list[test_idx])],\n",
    "                    max_epochs=CONFIG['epochs'], patience=CONFIG['patience'],eval_metric=[\"auc\", \"accuracy\", F1_macro])\n",
    "\n",
    "        # save model\n",
    "        model_name = f\"./tabnet_esm_model_fold{fold+1}\"\n",
    "        model_path = os.path.join(MODEL_DIR_NAME, model_name)\n",
    "        model.save_model(model_path)\n",
    "\n",
    "        # save history\n",
    "        history = model.history\n",
    "        loss, auc, acc, f1 = history['loss'], history['val_0_auc'], history['val_0_accuracy'], history['val_0_f1_macro']\n",
    "        \n",
    "        best_loss, best_f1 = 0, 0\n",
    "\n",
    "        for idx, (l, au, ac, f) in enumerate(zip(loss, auc, acc, f1)):\n",
    "            if (idx+1)%5==0:\n",
    "                fw.write(f\":Epoch {idx+1:2d} :: loss {l:.6f}\\t val Auc {au:.6f}\\t val Acc {ac:.6f}\\t val F1-macro: {f:.6f}\\n\")\n",
    "            if best_f1 < f:\n",
    "                best_f1 = f\n",
    "                best_loss = l\n",
    "        fw.write(f\"\\n::BEST VALID::  loss : {best_loss:.6f}\\t F1-macro: {best_f1:6f}:\\n\\n\")\n",
    "\n",
    "        avg_loss += best_loss\n",
    "        avg_f1 += best_f1\n",
    "        del model\n",
    "\n",
    "    fw.write(f\"\\n:== AVG 10 FOLD  loss : {avg_loss/5:.6f}\\t F1-macro: {avg_f1/5:6f} ==:\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model inference with 5-fold tabnet models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Predict Label Dist : [107563  13381]\n"
     ]
    }
   ],
   "source": [
    "preds_logits = np.zeros((len(df_test), 2))\n",
    "\n",
    "for fold in range(CONFIG['fold']):\n",
    "    model_path = os.path.join(MODEL_DIR_NAME, f\"tabnet_esm_model_fold{fold+1}.zip\")\n",
    "    tabnet_model = TabNetClassifier(**tabnet_params)\n",
    "    tabnet_model.load_model(model_path)\n",
    "\n",
    "    preds_logits+= tabnet_model.predict_proba(test_tab_net_feature_list)\n",
    "\n",
    "preds_logits/=5.\n",
    "preds = preds_logits[:, 1]\n",
    "pred_label = np.where(preds>CONFIG['threshold'], 1, 0)\n",
    "uni, cnt = np.unique(pred_label, return_counts=True)\n",
    "print(f\"Predict Label Dist : {cnt}\")\n",
    "\n",
    "# submit = pd.read_csv('../dataset/sample_submission.csv')\n",
    "submit = pd.read_csv('/data/sample_submission.csv')\n",
    "submit['label'] = pred_label\n",
    "submit.to_csv('./tabnet_submit.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('daicon')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "54d989783971caa44dc5b922ab59addd026782fc627ea79516231aafa74f47a5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
