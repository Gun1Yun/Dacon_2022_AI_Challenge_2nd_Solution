{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference만 하는 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import random\n",
    "import pickle\n",
    "from itertools import product\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import esm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MaxAbsScaler, Normalizer, RobustScaler, StandardScaler,MinMaxScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "from pytorch_tabnet.metrics import Metric\n",
    "\n",
    "# set visible device\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"0\"\n",
    "\n",
    "CONFIG = {\n",
    "    'n_worker':16,\n",
    "    # ESM embedding\n",
    "    'epitope_max_len':128,\n",
    "    'antigen_max_len':64,\n",
    "    'epitope_batch_size':50,\n",
    "    'antigen_batch_size':100,\n",
    "    # Feature extraction\n",
    "    'CT_CTD_features' : False,\n",
    "    'CNT_features' : False,\n",
    "    'CT_CTD_PCA':0,\n",
    "    'CNT_PCA':0,\n",
    "    # Tabnet model\n",
    "    'epochs' : 100,\n",
    "    'patience' : 20,\n",
    "    'learning_rate':2e-2,\n",
    "    'weight_decay':1e-5,\n",
    "    'threshold':0.5,\n",
    "    'seed':42,\n",
    "    'fold':5\n",
    "}\n",
    "\n",
    "# seed setting\n",
    "def seed_everything(seed:int):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(CONFIG['seed']) # Seed 고정\n",
    "\n",
    "# tabnet params\n",
    "tabnet_params = dict(\n",
    "    n_d = 64,   # 8 to 64\n",
    "    n_a = 128,  # n_d = n_a usally good\n",
    "    n_steps = 1,\n",
    "    gamma = 1.3,\n",
    "    lambda_sparse = 0,\n",
    "    n_independent = 2,\n",
    "    n_shared = 1,\n",
    "    optimizer_fn = optim.Adam,\n",
    "    optimizer_params = dict(lr = CONFIG['learning_rate'], weight_decay = CONFIG['weight_decay']),\n",
    "    mask_type = \"entmax\",\n",
    "    scheduler_params = dict(\n",
    "        mode = \"min\", patience = 5, min_lr = 1e-5, factor = 0.9),\n",
    "    scheduler_fn = ReduceLROnPlateau,\n",
    "    seed = CONFIG[\"seed\"],\n",
    "    verbose = 5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preprocessing(data_type, epitope_data, left_anti, right_anti, new_df):\n",
    "    tab_net_feature_list = []\n",
    "    feature_list = []\n",
    "\n",
    "    features_cols = [\"assay_method_technique\", \"assay_group\", \"disease_type\", \"disease_state\", \"reference_date\", \"reference_journal\", \"reference_title\"]\n",
    "    feature_data = enc.transform(new_df[features_cols]).toarray()\n",
    "\n",
    "    # sort by id\n",
    "    epitope_data.sort()\n",
    "    left_anti.sort()\n",
    "    right_anti.sort()\n",
    "    \n",
    "    for df_id, epi_embs, left_embs, right_embs, feature in tqdm(zip(new_df['id'], epitope_data, left_anti, right_anti, feature_data)):\n",
    "        if not df_id == epi_embs[0]:\n",
    "            print(\"Not matched ID\")\n",
    "        tab_net_features = np.append(epi_embs[1], left_embs[1])\n",
    "        tab_net_features = np.append(tab_net_features , right_embs[1])\n",
    "        tab_net_features = np.append(tab_net_features , feature)\n",
    "        tab_net_feature_list.append(tab_net_features)\n",
    "    \n",
    "    label_list = None\n",
    "    if data_type != 'test':\n",
    "        label_list = []\n",
    "        for label in new_df['label']:\n",
    "            label_list.append(label)\n",
    "        label_list = np.array(label_list)\n",
    "    print(f'{data_type} dataframe preprocessing was done.')\n",
    "\n",
    "    tab_net_feature_list = np.array(tab_net_feature_list)\n",
    "    \n",
    "    return tab_net_feature_list, label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============= Norm \n",
    "def norm_transform(datatype, data, scaler_name='z-score', scaler=None):\n",
    "    scaler_dict = {\n",
    "        'z-score':StandardScaler(),\n",
    "        'minmax':MinMaxScaler(),\n",
    "        'maxabs':MaxAbsScaler(),\n",
    "        'robust':RobustScaler(),\n",
    "        'norm':Normalizer()\n",
    "    }\n",
    "    \n",
    "    # use only train\n",
    "    if not datatype==\"test\":\n",
    "        scaler = scaler_dict[scaler_name]\n",
    "        scaled_train = scaler.fit_transform(data)\n",
    "        return scaled_train, scaler\n",
    "    else:\n",
    "        scaled_test = scaler.transform(data)\n",
    "        return scaled_test\n",
    "\n",
    "# ============= pca \n",
    "def pca_transform(datatype, data, n_comp=300, pca=None):\n",
    "    if not datatype==\"test\":\n",
    "        pca = PCA(n_components=n_comp, random_state=CONFIG[\"seed\"])\n",
    "        pca_train = pca.fit_transform(data)\n",
    "        print(f\"with {n_comp} components, pca variance ratio : {sum(pca.explained_variance_ratio_)}\")\n",
    "        return pca_train, pca\n",
    "    else:\n",
    "        pca_test = pca.transform(data)\n",
    "        return pca_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "361d8b00fe074a398fcd180cebd1a89e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test dataframe preprocessing was done.\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Predict Label Dist : [107563  13381]\n"
     ]
    }
   ],
   "source": [
    "# TRAIN_DATA_PATH = \"../dataset/train.csv\"\n",
    "# TEST_DATA_PATH = \"../dataset/test.csv\"\n",
    "TRAIN_DATA_PATH = \"/data/train.csv\"\n",
    "TEST_DATA_PATH = \"/data/dataset/test.csv\"\n",
    "EPITOPE_EMB_PATH = \"./Embeddings/test_epitope_embeddings.pkl\"\n",
    "LEFT_EMB_PATH = \"./Embeddings/test_left_embeddings.pkl\"\n",
    "RIGHT_EMB_PATH = \"./Embeddings/test_right_embeddings.pkl\"\n",
    "\n",
    "MODEL_DIR_NAME = \"./Tabnet_ESM_models\"\n",
    "\n",
    "# load train data\n",
    "df_train = pd.read_csv(TRAIN_DATA_PATH)\n",
    "df_test = pd.read_csv(TEST_DATA_PATH)\n",
    "\n",
    "# label one hot encoding\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "features_cols = [\"assay_method_technique\", \"assay_group\", \"disease_type\", \"disease_state\", \"reference_date\", \"reference_journal\", \"reference_title\"]\n",
    "enc.fit(df_train[features_cols])\n",
    "\n",
    "# load esm embeddings\n",
    "with open(EPITOPE_EMB_PATH, \"rb\") as fr:\n",
    "    epitope_data = pickle.load(fr)\n",
    "with open(LEFT_EMB_PATH, \"rb\") as fr:\n",
    "    left_anti_data = pickle.load(fr)\n",
    "with open(RIGHT_EMB_PATH, \"rb\") as fr:\n",
    "    right_anti_data = pickle.load(fr)\n",
    "\n",
    "# processing embedding features\n",
    "test_tab_net_feature_list, label_list = get_preprocessing('test', epitope_data, left_anti_data, right_anti_data, df_test)\n",
    "\n",
    "# CT-CTD features ===\n",
    "if CONFIG['CT_CTD_features']:\n",
    "    TEST_CT_CTD_PATH = f\"test_epitope_antigen_CTCTD.pkl\"\n",
    "    CT_CTD_SCALER = f\"CTCTD_scaler.pkl\"\n",
    "    CT_CTD_PCA = f\"CTCTD_pca.pkl\"\n",
    "    CT_CTD_DIR = \"CT_CTD_Features\"\n",
    "\n",
    "    test_ct_ctd_path = os.path.join(CT_CTD_DIR, TEST_CT_CTD_PATH)\n",
    "    scaler_ct_ctd_path = os.path.join(CT_CTD_DIR, CT_CTD_SCALER)\n",
    "    pca_ct_ctd__path = os.path.join(CT_CTD_DIR, CT_CTD_PCA)\n",
    "\n",
    "    with open(test_ct_ctd_path, \"rb\") as fr:\n",
    "        test_ct_ctd_features= pickle.load(fr)\n",
    "        \n",
    "    with open(scaler_ct_ctd_path, \"rb\") as fr:\n",
    "        scaler = pickle.load(fr)\n",
    "\n",
    "    test_ct_ctd_features = norm_transform(\"test\", test_ct_ctd_features, \"minmax\", scaler)\n",
    "\n",
    "    # PCA\n",
    "    if not CONFIG['CT_CTD_PCA']==0:\n",
    "        n_comps = CONFIG['CT_CTD_PCA']\n",
    "        before_features = test_ct_ctd_features.shape[1]\n",
    "\n",
    "        with open(pca_ct_ctd__path, \"rb\") as fr:\n",
    "            pca = pickle.load(fr)\n",
    "\n",
    "        test_ct_ctd_features = pca_transform(\"test\", test_ct_ctd_features, n_comps, pca)\n",
    "        # need to save pca scaler\n",
    "        print(f\"CT_CTD feature decomposition finished {before_features} -> {n_comps}\")\n",
    "\n",
    "    test_tab_net_feature_list = np.concatenate((test_ct_ctd_features, test_tab_net_feature_list), axis=1)\n",
    "\n",
    "# CNT featuers ===\n",
    "if CONFIG['CNT_features']:\n",
    "    TEST_CNT_PATH = f\"test_epitope_antigen_CNT.pkl\"\n",
    "    CNT_SCALER = f\"CNT_scaler.pkl\"\n",
    "    CNT_PCA = f\"CNT_pca.pkl\"\n",
    "    CNT_DIR = \"CNT_Features\"\n",
    "\n",
    "    test_cnt_path = os.path.join(CNT_DIR, TEST_CNT_PATH)\n",
    "    scaler_cnt_path = os.path.join(CNT_DIR, CNT_SCALER)\n",
    "    pca_cnt_path = os.path.join(CNT_DIR, CNT_PCA)\n",
    "    \n",
    "\n",
    "    with open(test_cnt_path, \"rb\") as fr:\n",
    "        test_cnt_features= pickle.load(fr)\n",
    "    # Norm\n",
    "    with open(scaler_cnt_path, \"rb\") as fr:\n",
    "        scaler = pickle.load(fr)\n",
    "\n",
    "    test_cnt_features = norm_transform(\"test\", test_cnt_features, \"minmax\", scaler)\n",
    "\n",
    "\n",
    "    # PCA\n",
    "    if not CONFIG['CNT_PCA']==0:\n",
    "        n_comps = CONFIG['CNT_PCA']\n",
    "        before_features = test_cnt_features.shape[1]\n",
    "\n",
    "        with open(pca_cnt_path, \"rb\") as fr:\n",
    "            pca = pickle.load(fr)\n",
    "\n",
    "        test_cnt_features = pca_transform(\"test\", test_cnt_features, n_comps, pca)\n",
    "        print(f\"CNT feature decomposition finished {before_features.shape[1]} -> {n_comps}\")\n",
    "\n",
    "    test_tab_net_feature_list = np.concatenate((test_cnt_features, test_tab_net_feature_list), axis=1)\n",
    "\n",
    "\n",
    "preds_logits = np.zeros((len(df_test), 2))\n",
    "\n",
    "for fold in range(CONFIG['fold']):\n",
    "    model_path = os.path.join(MODEL_DIR_NAME, f\"tabnet_esm_model_fold{fold+1}.zip\")\n",
    "    tabnet_model = TabNetClassifier(**tabnet_params)\n",
    "    tabnet_model.load_model(model_path)\n",
    "\n",
    "    preds_logits+= tabnet_model.predict_proba(test_tab_net_feature_list)\n",
    "\n",
    "preds_logits/=5.\n",
    "preds = preds_logits[:, 1]\n",
    "pred_label = np.where(preds>CONFIG['threshold'], 1, 0)\n",
    "uni, cnt = np.unique(pred_label, return_counts=True)\n",
    "print(f\"Predict Label Dist : {cnt}\")\n",
    "\n",
    "submit = pd.read_csv('/data/sample_submission.csv')\n",
    "submit['label'] = pred_label\n",
    "submit.to_csv('./tabnet_submit.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('daicon')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "54d989783971caa44dc5b922ab59addd026782fc627ea79516231aafa74f47a5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
